{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T11:04:48.681077Z",
     "start_time": "2017-11-15T11:04:48.677067Z"
    }
   },
   "source": [
    "# Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T11:04:55.400939Z",
     "start_time": "2017-11-15T11:04:55.397931Z"
    }
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T11:05:29.836473Z",
     "start_time": "2017-11-15T11:05:29.832457Z"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:14.981987Z",
     "start_time": "2018-02-22T19:50:04.799239Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.optimize as optimize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from E4525_ML.multiclass_logistic import LogisticGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:14.982987Z",
     "start_time": "2018-02-22T19:50:14.981987Z"
    }
   },
   "outputs": [],
   "source": [
    "seed=12759\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Multiclass Loss and Gradient\n",
    "\n",
    "### Clas Probabilities\n",
    "For multi-label classification problems we asume $Y|_X$ is a categorical variable with $K$ classes.\n",
    "\n",
    "The logistic model asumes that the categorical variable **class probabilities** are\n",
    "\\begin{equation}\n",
    "\t\t\\theta_k(x) = \\textrm{softmax}_k(\\eta(x))\n",
    "\\end{equation}\n",
    "were  we define  the `logits` variables $\\eta$ as depending linearly on $x$\n",
    "\\begin{equation}\n",
    "\t\t\\eta(x) = W^T x + b\n",
    "\\end{equation}\n",
    "or, writing all the indexes explicitly\n",
    "\\begin{equation}\n",
    "    \\eta_k(x) = \\sum_d W_{d,k} x_d + b_k\n",
    "\\end{equation}\n",
    "where we have explicitly highlited the bias term $b$, instead of absorbing it on $W$.\n",
    "\n",
    "The function `softmax` is defined as\n",
    "\\begin{equation}\n",
    "\t\\theta_k(\\eta)=\\textrm{softmax}_k(\\eta) = \\frac{ e^{\\eta_k}}{\\sum_{k'} e^{\\eta_{k'}}}\n",
    "\\end{equation}\n",
    "\n",
    "To avoid numerical accuracy problems with the exponential of very large numbers, we multiple and divide\n",
    "the `softmax` expresion by the exponential of \n",
    "$$\n",
    "    \\bar{\\eta} = \\max_k \\eta_k\n",
    "$$\n",
    "so that $\\theta_k$ becomes\n",
    "\\begin{equation}\n",
    "\t\\theta_k(\\eta)=\\textrm{softmax}_k(\\eta) = \\frac{ e^{\\eta_k-\\bar{\\eta}}}{\\sum_{k'} e^{\\eta_{k'}-\\bar{\\eta}}}\n",
    "\\end{equation}\n",
    "where the largest term in the denominator sum will be $1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.026187Z",
     "start_time": "2018-02-22T19:50:14.983987Z"
    }
   },
   "outputs": [],
   "source": [
    "def logisticClassProbability(X,b,W):\n",
    "    logits=X.dot(W.T)+b\n",
    "    elogits=np.exp(logits-logits.max(axis=1)[:,np.newaxis]) # make calculation more stable numerically\n",
    "    elogits_sum=elogits.sum(axis=1)\n",
    "    class_probs=elogits/elogits_sum[:,np.newaxis]\n",
    "    return class_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Cross Entropy Loss \n",
    "\n",
    "  The maximum likelihood loss is given by\n",
    "  \n",
    "  $$\n",
    "      E(b,W;\\{x_i,y_i\\}) = -\\sum_i \\sum_k z_{i,k} \\log \\theta_k(\\eta(x_i))\n",
    "  $$\n",
    "  \n",
    "  [Note] We have dropped a factor of $\\frac{1}{N}$ from the loss relative to the class notes that does not affect the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.026187Z",
     "start_time": "2018-02-22T19:50:14.983987Z"
    }
   },
   "outputs": [],
   "source": [
    "def logisticLoss(X,Z,b,W):\n",
    "    class_probs=logisticClassProbability(X,b,W)\n",
    "    loss= np.sum(-(Z*np.log(np.maximum(class_probs,1e-10))).sum(axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Gradient\n",
    "For optimization we will also need the loss gradient\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial E}{\\partial b_k} &= \\sum_i \\theta_k(x_i) - z_{i,k} \\\\\n",
    "    \\frac{\\partial E}{\\partial W_{d,k}} &= \\sum_i \\left( \\theta_k(x_i) - z_{i,k}\\right) x_{i,d}\n",
    " \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.026187Z",
     "start_time": "2018-02-22T19:50:14.983987Z"
    }
   },
   "outputs": [],
   "source": [
    "def logisticGradient(X,Z,b,W):\n",
    "    class_probs=logisticClassProbability(X,b,W)\n",
    "    delta=(Z-class_probs)\n",
    "    return -delta.sum(axis=0),-np.dot(delta.T,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function for optimization\n",
    "\n",
    "We will include a regularization penalty\n",
    "$$\n",
    "    \\Omega(W) = \\frac{\\lambda}{2}\\sum_{k,d} W_{d,k}^2\n",
    "$$ \n",
    "in the objective function. We do not include a penalty on $b$ to assure that the class predicted class probabilities match the empirical class frequencies\n",
    "$$\n",
    "        \\sum_i \\theta_k(x_i) = \\sum_i z_{i,k}\n",
    "$$\n",
    "With that extra term we have to optimize the objective function $L$ given by\n",
    "\n",
    "\\begin{align}\n",
    "    L(b,W;\\{x_i,y_i\\}) &= E(b,W;\\{x_i,y_i\\}) + \\lambda \\frac{1}{2}\\sum_{d,k} W_{k,d}^2 \\\\\n",
    "    \\frac{\\partial L}{\\partial b_k} &= \\frac{\\partial E}{\\partial b_k} \\\\\n",
    "    \\frac{\\partial L}{\\partial W_{d,k}} &= \\frac{\\partial E}{\\partial W_{d,k}} + \\lambda W_{k,d}\n",
    "\\end{align}\n",
    "\n",
    "[Note] the optimization package optimizes over a single vector $x$, so we need to flatten $b,W$ into\n",
    "$$\n",
    "x = ( b_1, \\dots b_K,W_{1,1,}, \\dots W_{D,K} )\n",
    "$$\n",
    "and similarly for gradients\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial x} = \\left( \n",
    "       \\frac{\\partial L}{\\partial b_1},\\dots, \\frac{\\partial L}{\\partial b_K},\n",
    "       \\frac{\\partial L}{\\partial W_{1,1}},\\dots,\\frac{\\partial L}{\\partial W_{D,K}}\n",
    "       \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.062387Z",
     "start_time": "2018-02-22T19:50:15.028687Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_func(x0,X,Z,penalty):\n",
    "    D=X.shape[1]\n",
    "    K=Z.shape[1]\n",
    "    b=x0[:K]\n",
    "    W=x0[K:].reshape((K,D))\n",
    "    loss=logisticLoss(X,Z,b,W)\n",
    "    if penalty>0:\n",
    "        loss+=0.5*penalty*(W**2).sum()\n",
    "    return loss\n",
    "\n",
    "def grad_func(x0,X,Z,penalty):\n",
    "    D=X.shape[1]\n",
    "    K=Z.shape[1]\n",
    "    b=x0[:K]\n",
    "    W=x0[K:].reshape((K,D))\n",
    "    gradb,gradW=logisticGradient(X,Z,b,W)\n",
    "    if penalty>0: # optimization, do not perform the sum if penalty==0 \n",
    "        gradW+=penalty*W\n",
    "    # we return first the gradient relative to $b$ and then to $W$\n",
    "    return np.concatenate([gradb,gradW.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T11:04:06.724557Z",
     "start_time": "2017-11-15T11:04:06.718541Z"
    }
   },
   "source": [
    "### Check Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.064887Z",
     "start_time": "2018-02-22T19:50:15.062387Z"
    }
   },
   "outputs": [],
   "source": [
    "X=np.array([[0,1,2,3,4],\n",
    "            [1,2,3,4,5],\n",
    "            [2,3,4,5,6],\n",
    "            [3,4,5,6,7],          \n",
    "           ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.084887Z",
     "start_time": "2018-02-22T19:50:15.067387Z"
    }
   },
   "outputs": [],
   "source": [
    "Z=np.array([[0,0,1],\n",
    "            [1,0,0],\n",
    "            [0,1,0],\n",
    "            [1,0,0]\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.104887Z",
     "start_time": "2018-02-22T19:50:15.084887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W=np.array([[1,0.1,0.1,0.0,0.1],\n",
    "            [0.1,1,0.2,0.1,0.3],\n",
    "            [-.1,-0.2,1.2,0.2,-2]])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.124887Z",
     "start_time": "2018-02-22T19:50:15.107387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.array([0.0,-2.0,2.0])\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.142387Z",
     "start_time": "2018-02-22T19:50:15.124887Z"
    }
   },
   "outputs": [],
   "source": [
    "penalty=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.162387Z",
     "start_time": "2018-02-22T19:50:15.142387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. , -2. ,  2. ,  1. ,  0.1,  0.1,  0. ,  0.1,  0.1,  1. ,  0.2,\n",
       "        0.1,  0.3, -0.1, -0.2,  1.2,  0.2, -2. ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0=np.concatenate([b,W.ravel()])\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.219887Z",
     "start_time": "2018-02-22T19:50:15.164387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.33098116e-01,  1.72320185e+00, -9.90103732e-01, -2.51464574e+00,\n",
       "       -3.24774385e+00, -3.98084223e+00, -4.71394035e+00, -5.44703838e+00,\n",
       "        2.51370160e+00,  4.23690345e+00,  5.96010521e+00,  7.68330723e+00,\n",
       "        9.40650899e+00,  9.44133660e-04, -9.89159599e-01, -1.97926324e+00,\n",
       "       -2.96936697e+00, -3.95947053e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize.approx_fprime(x0,val_func,1e-8,X,Z,penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.227387Z",
     "start_time": "2018-02-22T19:50:15.219887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8437902120324466e-07"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize.check_grad(val_func,grad_func,x0,X,Z,penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T11:16:04.725087Z",
     "start_time": "2017-11-15T11:16:04.721077Z"
    },
    "collapsed": true
   },
   "source": [
    "## Generate Random Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T09:54:26.217860Z",
     "start_time": "2017-11-16T09:54:26.214849Z"
    }
   },
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.244887Z",
     "start_time": "2018-02-22T19:50:15.227387Z"
    }
   },
   "outputs": [],
   "source": [
    "D=2\n",
    "K=4\n",
    "N=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.263387Z",
     "start_time": "2018-02-22T19:50:15.244887Z"
    }
   },
   "outputs": [],
   "source": [
    "mu=np.array([0.0,0.0])\n",
    "cov=np.array([[1.0,0.7],\n",
    "              [0.7,1.0]\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.283387Z",
     "start_time": "2018-02-22T19:50:15.264387Z"
    }
   },
   "outputs": [],
   "source": [
    "W0=np.array([\n",
    "             [10.0,0.0 ],\n",
    "             [0.0, 5.0],\n",
    "             [5.0,5.0],\n",
    "             [5.0,-5.0]\n",
    "            ]\n",
    "           )\n",
    "b0=np.array([0.,0.0,2.,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.310887Z",
     "start_time": "2018-02-22T19:50:15.285387Z"
    }
   },
   "outputs": [],
   "source": [
    "bW0=np.c_[b0,W0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.330887Z",
     "start_time": "2018-02-22T19:50:15.310887Z"
    }
   },
   "outputs": [],
   "source": [
    "X=random.multivariate_normal(mu,cov,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.353387Z",
     "start_time": "2018-02-22T19:50:15.330887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10871596, 0.22081613])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate samples that follow the logistic distribution. Given $x$, the one hot encoding $z$ of $y$ is generated as\n",
    "$$\n",
    "    Z(x) \\sim \\textrm{Categorical}(\\theta(x))\n",
    "$$\n",
    "where the categorical distribution parameters are generated as\n",
    "$$\n",
    "    \\theta_k(x) = \\textrm{softmax}(\\sum_{k,d}x_d W_{k,d} +b_k)=\\textrm{softmax}(x W^T +b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.367887Z",
     "start_time": "2018-02-22T19:50:15.353387Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_logistic_multinomial(X,W,b):\n",
    "    X1=np.c_[np.ones(len(X)),X]\n",
    "    bW=np.c_[b,W]\n",
    "    nu=np.dot(X1,bW.T)\n",
    "    enu=np.exp(nu)\n",
    "    enu_sum=enu.sum(axis=1)\n",
    "    pi=enu/enu_sum[:,np.newaxis]\n",
    "    Z=np.empty_like(pi)\n",
    "    for i1 in range(len(pi)):\n",
    "        Z[i1]=random.multinomial(1,pi[i1],1)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.390887Z",
     "start_time": "2018-02-22T19:50:15.368887Z"
    }
   },
   "outputs": [],
   "source": [
    "Z=generate_logistic_multinomial(X,W0,b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.411887Z",
     "start_time": "2018-02-22T19:50:15.392887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13, 0.14, 0.47, 0.26])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.435387Z",
     "start_time": "2018-02-22T19:50:15.415387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 2, 0, 1, 1, 3, 1, 2, 2, 3, 2, 2, 2, 2, 3, 0, 0, 0, 3, 2,\n",
       "       2, 2, 2, 2, 3, 3, 2, 1, 3, 3, 2, 2, 2, 3, 2, 3, 0, 2, 2, 2, 2, 0,\n",
       "       2, 0, 2, 3, 2, 3, 1, 3, 2, 1, 0, 3, 1, 2, 2, 3, 2, 3, 2, 3, 2, 3,\n",
       "       3, 0, 1, 2, 2, 0, 2, 0, 3, 2, 2, 3, 2, 2, 2, 1, 3, 1, 2, 1, 1, 3,\n",
       "       3, 0, 2, 2, 3, 2, 3, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=Z.argmax(axis=1)\n",
    "Y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:15.457887Z",
     "start_time": "2018-02-22T19:50:15.435387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.345100</td>\n",
       "      <td>2.007299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700666</td>\n",
       "      <td>-0.055637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.881124</td>\n",
       "      <td>0.036733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.773288</td>\n",
       "      <td>0.408596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.128024</td>\n",
       "      <td>0.890464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Y        X1        X2\n",
       "0  2.0  1.345100  2.007299\n",
       "1  0.0  0.700666 -0.055637\n",
       "2  1.0 -0.881124  0.036733\n",
       "3  2.0  0.773288  0.408596\n",
       "4  0.0  2.128024  0.890464"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(np.c_[Y,X],columns=[\"Y\",\"X1\",\"X2\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.052387Z",
     "start_time": "2018-02-22T19:50:15.457887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x7fd1b75378d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1b7537ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pairplot(data,vars=[\"X1\",\"X2\"],hue=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T09:53:46.877305Z",
     "start_time": "2017-11-16T09:53:46.873295Z"
    }
   },
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.059887Z",
     "start_time": "2018-02-22T19:50:16.052387Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test=random.multivariate_normal(mu,cov,size=N)\n",
    "Z_test=generate_logistic_multinomial(X_test,W0,b0)\n",
    "Y_test=Z_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T16:06:41.617552Z",
     "start_time": "2017-12-04T16:06:41.614552Z"
    }
   },
   "source": [
    "## Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic Regression using `scipy.optimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.075387Z",
     "start_time": "2018-02-22T19:50:16.059887Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_logistic_weights_scipy(X,Z,b,W,penalty=0,\n",
    "                                    method=\"newton-cg\",\n",
    "                                    tol=1e-16,\n",
    "                                    max_iter=100):\n",
    "    D=X.shape[1]\n",
    "    K=Z.shape[1]\n",
    "    x0=np.concatenate((b,W.ravel()))\n",
    "   \n",
    "    fit=optimize.minimize(val_func, x0, args=(X,Z,penalty),jac=grad_func,\n",
    "             method=method,   \n",
    "             tol=tol)\n",
    "    x1=fit.x\n",
    "    b=x1[:K]\n",
    "    W=x1[K:].reshape((K,D))\n",
    "    return b,W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test Scipy Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.094387Z",
     "start_time": "2018-02-22T19:50:16.076387Z"
    }
   },
   "outputs": [],
   "source": [
    "D=X.shape[1]\n",
    "K=Z.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.118387Z",
     "start_time": "2018-02-22T19:50:16.095387Z"
    }
   },
   "outputs": [],
   "source": [
    "b_guess=np.zeros(K)\n",
    "W_guess=random.normal(0,1,(K,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.189387Z",
     "start_time": "2018-02-22T19:50:16.118387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93690446  1.91172419  4.85650468 -7.70522182] [[  8.22743327   2.60067161]\n",
      " [ -7.74696288  10.68312869]\n",
      " [ -0.17658796  10.92849542]\n",
      " [  0.7383922  -24.82686834]]\n"
     ]
    }
   ],
   "source": [
    "b,W=optimize_logistic_weights_scipy(X,Z,b_guess,W_guess,penalty=0,tol=1e-4)\n",
    "print(b,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Optimizer\n",
    "\n",
    "For each **epoch** ( a complete optimization run through all the training data) we:\n",
    "1. Permute the input data.\n",
    "2. Break input data in small size **mini-batches**\n",
    "3. Update parameters on each mini-bath using the gradient descent update rule.\n",
    "4. For convenience, we report losses in both training and valuation 10 times (once every 100 epochs) during optimization.\n",
    "\n",
    "We repeate this procedure for `max_iter` number of epochs (**complete runs through all the training data**), or until we stop improving the lost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.290486Z",
     "start_time": "2018-02-22T19:50:16.190387Z"
    }
   },
   "outputs": [],
   "source": [
    "def report_function(e,params,X,Z,X_val,Z_val,penalty):\n",
    "    D=X.shape[1]\n",
    "    K=Z.shape[1]\n",
    "    b=params[:K]\n",
    "    W=params[K:].reshape((K,D))\n",
    "    loss=val_func(params,X,Z,penalty)\n",
    "    class_probs=logisticClassProbability(X,b,W)\n",
    "    Y_pred=class_probs.argmax(axis=1) \n",
    "    Y=Z.argmax(axis=1)\n",
    "    train_accuracy=np.mean(Y_pred==Y)\n",
    "    print(\"\\t\",e,\"Loss =\",loss,\"Train_Accuracy\",train_accuracy,end=\" \")\n",
    "    if not(X_val is None): # if we have a valuation set we can report\n",
    "                        # how well we are doing out of sample\n",
    "        val_e=val_func(params,X_val,Z_val,penalty)\n",
    "        class_probs=logisticClassProbability(X_val,b,W)\n",
    "        Y_pred=class_probs.argmax(axis=1)          \n",
    "        Y_val=Z_val.argmax(axis=1)     \n",
    "        val_accuracy=np.mean(Y_pred==Y_val)\n",
    "        print(\"Evaluation Loss =\",val_e,\"Accuracy =\",val_accuracy)\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "\n",
    "### Logistic Regression using Stochastic Gradient Descent\n",
    "def optimize_logistic_weights(X,Z,b,W,\n",
    "                            X_val=None,\n",
    "                            Z_val=None,\n",
    "                            penalty=0,\n",
    "                            learning_rate=0.01,\n",
    "                            tol=1e-8,\n",
    "                            max_iter=1000,\n",
    "                            batch_size=100,\n",
    "                            verbose=True\n",
    "                              ):\n",
    "    \n",
    "    D=X.shape[1]\n",
    "    K=Z.shape[1]\n",
    "    x=np.concatenate((b,W.ravel()))\n",
    "    if Z_val is not None:\n",
    "        Y_val=Z_val.argmax(axis=1)\n",
    "    N=len(X)\n",
    "    l0=val_func(x,X,Z,penalty)\n",
    "    for e in range(max_iter):\n",
    "        if (e%(max_iter//10)==0 and verbose):\n",
    "            report_function(e,x,X,Z,X_val,Z_val,penalty)\n",
    "        perm=random.permutation(N)\n",
    "        for i in range(0,N,batch_size):\n",
    "            Xb=X[perm[i:i+batch_size]]\n",
    "            Zb=Z[perm[i:i+batch_size]]\n",
    "            p=len(Xb)/N*penalty\n",
    "            grad=grad_func(x,Xb,Zb,p)\n",
    "            x-=learning_rate*grad\n",
    "        l=val_func(x,X,Z,penalty)\n",
    "       \n",
    "        d=np.abs(l-l0)\n",
    "        if d<tol*l0:\n",
    "            break\n",
    "        l0=l  \n",
    "    if verbose:\n",
    "        report_function(e,x,X,Z,X_val,Z_val,penalty)\n",
    "    b=x[:K]\n",
    "    W=x[K:].reshape((K,D))\n",
    "    return b,W   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.469486Z",
     "start_time": "2018-02-22T19:50:16.291486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0 Loss = 299.2921064941822 Train_Accuracy 0.05 Evaluation Loss = 273.1162446159898 Accuracy = 0.11\n",
      "\t 100 Loss = 72.34669765799127 Train_Accuracy 0.78 Evaluation Loss = 80.87929782907966 Accuracy = 0.7\n",
      "\t 200 Loss = 53.63530808024932 Train_Accuracy 0.84 Evaluation Loss = 63.85907129598698 Accuracy = 0.72\n",
      "\t 300 Loss = 45.8091298397824 Train_Accuracy 0.84 Evaluation Loss = 57.744296293227386 Accuracy = 0.76\n",
      "\t 400 Loss = 41.410711569026525 Train_Accuracy 0.85 Evaluation Loss = 54.553172968296245 Accuracy = 0.79\n",
      "\t 500 Loss = 38.53236686419029 Train_Accuracy 0.86 Evaluation Loss = 52.54732002480233 Accuracy = 0.81\n",
      "\t 600 Loss = 36.47627916788786 Train_Accuracy 0.86 Evaluation Loss = 51.15585696924866 Accuracy = 0.83\n",
      "\t 700 Loss = 34.92151578312621 Train_Accuracy 0.86 Evaluation Loss = 50.130045041659145 Accuracy = 0.83\n",
      "\t 800 Loss = 33.69772475180277 Train_Accuracy 0.86 Evaluation Loss = 49.34192724969121 Accuracy = 0.82\n",
      "\t 900 Loss = 32.705220657483146 Train_Accuracy 0.86 Evaluation Loss = 48.71811823919791 Accuracy = 0.82\n",
      "\t 999 Loss = 31.88139672874515 Train_Accuracy 0.87 Evaluation Loss = 48.21315101647232 Accuracy = 0.82\n",
      "[-0.69148881 -0.20066714  1.13433373 -0.24217779] [[ 3.53934867 -0.46679066]\n",
      " [-2.79687523  1.29146607]\n",
      " [ 0.49740589  2.64548399]\n",
      " [-0.19764703 -4.08475035]]\n"
     ]
    }
   ],
   "source": [
    "b,W=optimize_logistic_weights(X,Z, b_guess,W_guess,\n",
    "                             X_test, Z_test, \n",
    "                             penalty=0,learning_rate=0.001,batch_size=100,tol=1e-8)\n",
    "print(b,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Sample Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.474486Z",
     "start_time": "2018-02-22T19:50:16.471486Z"
    }
   },
   "outputs": [],
   "source": [
    "pi=logisticClassProbability(X,b,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.495486Z",
     "start_time": "2018-02-22T19:50:16.477486Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_pred=pi.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.519986Z",
     "start_time": "2018-02-22T19:50:16.497486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y_pred==Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Sample Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:52:20.197575Z",
     "start_time": "2018-02-22T19:52:20.196575Z"
    }
   },
   "outputs": [],
   "source": [
    "pi=logisticClassProbability(X_test,b,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:52:29.596831Z",
     "start_time": "2018-02-22T19:52:29.594831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred=pi.argmax(axis=1)\n",
    "np.mean(Y_pred==Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T09:52:43.841038Z",
     "start_time": "2017-11-16T09:52:43.837027Z"
    }
   },
   "source": [
    "### Stochastic Gradient Descent Logistic Classifier Class\n",
    "\n",
    "The convenience Stochastic Gradient descent classifier, sets defaults for most parameters, takes care of the one-hot encoding of $Y$, etc. and follows the sklearn `fit`/`predict` convention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.589486Z",
     "start_time": "2018-02-22T19:50:16.519986Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticGDClassifier:\n",
    "    def __init__(self,\n",
    "        penalty=0,\n",
    "        learning_rate=0.005,\n",
    "        batch_size=100,\n",
    "        tol=1e-4,\n",
    "        max_iter=100,\n",
    "        verbose=True\n",
    "    ):\n",
    "        self.penalty=penalty\n",
    "        self.learning_rate=learning_rate\n",
    "        self.batch_size=batch_size\n",
    "        self.tol=tol\n",
    "        self.max_iter=max_iter\n",
    "        self.verbose=verbose\n",
    "    def fit(self,X,Y,X_val=None,Y_val=None):\n",
    "        # X_val and Y_val are only used to report accuracy during optimization\n",
    "        # they do not affect the fitted W,b parameters\n",
    "        \n",
    "        \n",
    "        if X.ndim==1:\n",
    "            X=X.reshape(1,-1)\n",
    "        N,D=X.shape\n",
    "       \n",
    "        self.encoder=LabelEncoder()\n",
    "        y=self.encoder.fit_transform(Y)      \n",
    "        K=len(self.encoder.classes_)\n",
    "        Z=np.zeros((N,K),dtype=int)\n",
    "        Z[np.arange(N),y]=1\n",
    "        \n",
    "        \n",
    "        if not(X_val is None):\n",
    "            N_val=len(X_val)\n",
    "            y_val=self.encoder.transform(Y_val)  \n",
    "            Z_val=np.zeros((N_val,K),dtype=int)\n",
    "            Z_val[np.arange(N_val),y_val]=1\n",
    "        else: \n",
    "            Z_val=None\n",
    "\n",
    "        b_guess=np.zeros(K)\n",
    "        W_guess=random.normal(0,1,(K,D))/np.sqrt(D)\n",
    "        \n",
    "        self.b,self.W=optimize_logistic_weights(X,Z,b_guess,W_guess,\n",
    "                                            X_val=X_val,\n",
    "                                            Z_val=Z_val,\n",
    "                                            penalty=self.penalty,\n",
    "                                            learning_rate=self.learning_rate,\n",
    "                                            batch_size=self.batch_size,\n",
    "                                            tol=self.tol,\n",
    "                                            max_iter=self.max_iter,\n",
    "                                            verbose=self.verbose\n",
    "                                            )\n",
    "        \n",
    "    def predict(self,X):\n",
    "        if X.ndim==1:\n",
    "            X=X.reshape(1,-1)\n",
    "        N,D=X.shape   \n",
    "        class_probs=logisticClassProbability(X,self.b,self.W)        \n",
    "        y=class_probs.argmax(axis=1)\n",
    "        return self.encoder.inverse_transform(y)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.609486Z",
     "start_time": "2018-02-22T19:50:16.592486Z"
    }
   },
   "outputs": [],
   "source": [
    "model=LogisticGDClassifier(penalty=0,learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.661986Z",
     "start_time": "2018-02-22T19:50:16.609486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0 Loss = 128.29894338365216 Train_Accuracy 0.57 Evaluation Loss = 132.15145084663087 Accuracy = 0.48\n",
      "\t 10 Loss = 63.757558529039876 Train_Accuracy 0.77 Evaluation Loss = 72.95425487076339 Accuracy = 0.71\n",
      "\t 20 Loss = 50.8762247704224 Train_Accuracy 0.84 Evaluation Loss = 62.338037286007626 Accuracy = 0.76\n",
      "\t 30 Loss = 44.49847744119103 Train_Accuracy 0.85 Evaluation Loss = 57.37379469571593 Accuracy = 0.8\n",
      "\t 40 Loss = 40.62119802099522 Train_Accuracy 0.85 Evaluation Loss = 54.446865483458616 Accuracy = 0.81\n",
      "\t 50 Loss = 37.98487085923187 Train_Accuracy 0.86 Evaluation Loss = 52.51152480074172 Accuracy = 0.82\n",
      "\t 60 Loss = 36.06150792898946 Train_Accuracy 0.86 Evaluation Loss = 51.13786421370634 Accuracy = 0.83\n",
      "\t 70 Loss = 34.58851097144671 Train_Accuracy 0.86 Evaluation Loss = 50.11446132859266 Accuracy = 0.83\n",
      "\t 80 Loss = 33.41956450699294 Train_Accuracy 0.87 Evaluation Loss = 49.32469810892781 Accuracy = 0.82\n",
      "\t 90 Loss = 32.46626878173838 Train_Accuracy 0.86 Evaluation Loss = 48.6987752718602 Accuracy = 0.82\n",
      "\t 99 Loss = 31.67187136637704 Train_Accuracy 0.86 Evaluation Loss = 48.19226958738501 Accuracy = 0.82\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,Y,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.665986Z",
     "start_time": "2018-02-22T19:50:16.661986Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manel/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred=model.predict(X_test)\n",
    "np.mean(Y_pred==Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes Error Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data was generated using a logistic like process with parameters $W0$ and $b0$, the best\n",
    "we han hope to achive is an error rate equal to the one computed with those parameters. \n",
    "\n",
    "This is known as the **Bayes error rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.684986Z",
     "start_time": "2018-02-22T19:50:16.666986Z"
    }
   },
   "outputs": [],
   "source": [
    "model0=copy(model)\n",
    "model0.b=b0\n",
    "model0.W=W0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.705986Z",
     "start_time": "2018-02-22T19:50:16.685986Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manel/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred0=model0.predict(X_test)\n",
    "np.mean(Y_pred0==Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T16:50:03.485552Z",
     "start_time": "2017-12-04T16:50:03.478552Z"
    }
   },
   "source": [
    "The best prediction accuracy we could expect is 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T17:24:26.823552Z",
     "start_time": "2017-12-04T17:24:26.820552Z"
    },
    "collapsed": true
   },
   "source": [
    "### Comparison to SKLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T17:26:09.147552Z",
     "start_time": "2017-12-04T17:26:09.141552Z"
    }
   },
   "source": [
    "We can also use `sklearn` to create a multi class classifier, but **be careful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T20:00:54.722800Z",
     "start_time": "2018-02-22T20:00:54.717800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression()\n",
    "model.fit(X,Y)\n",
    "Y_pred=model.predict(X_test)\n",
    "np.mean(Y_pred==Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be default, the `sklearn` classifier uses regularization ($C=1$, equivalent to $\\lambda=1$), and solves a **one versus rest** classification problem, not the full multi-class logistic regression problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T17:28:48.858552Z",
     "start_time": "2017-12-04T17:28:48.852552Z"
    }
   },
   "source": [
    "**Corrected comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T19:50:16.755486Z",
     "start_time": "2018-02-22T19:50:16.730486Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manel/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression(C=1e30,multi_class=\"multinomial\",solver=\"sag\")\n",
    "model.fit(X,Y)\n",
    "Y_pred=model.predict(X_test)\n",
    "np.mean(Y_pred==Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now results are equivalent to our own, and as good as they can possible be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
