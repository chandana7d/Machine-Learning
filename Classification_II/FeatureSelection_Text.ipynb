{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection  with the $\\chi^2$ test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When handling classification with large numbers of potential categorical explanatory variables (for example text) we would need to find a method to **select** the most relevant variables.\n",
    " \n",
    "In possible technique is the $\\chi^2$ test were we search for variables that are the least independent of  class labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:51.794607Z",
     "start_time": "2018-10-11T14:31:42.825607Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from E4525_ML import plots\n",
    "from E4525_ML.text import stem_tokenizer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:51.799607Z",
     "start_time": "2018-10-11T14:31:51.795607Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data_dir=r\"../../raw/C50/C50train\"\n",
    "test_dir    =r\"../../raw/C50/C50test\"\n",
    "data_dir=r\"../../data/C50\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus (List of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:51.998607Z",
     "start_time": "2018-10-11T14:31:51.802607Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/253868newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/305692newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/340736newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/140340newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/126593newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      filename        label\n",
       "document_id                                                                \n",
       "0            ../../raw/C50/C50train/ScottHillis/253868newsM...  ScottHillis\n",
       "1            ../../raw/C50/C50train/ScottHillis/305692newsM...  ScottHillis\n",
       "2            ../../raw/C50/C50train/ScottHillis/340736newsM...  ScottHillis\n",
       "3            ../../raw/C50/C50train/ScottHillis/140340newsM...  ScottHillis\n",
       "4            ../../raw/C50/C50train/ScottHillis/126593newsM...  ScottHillis"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_filename=data_dir+\"/C50_documents.csv\"\n",
    "documents=pd.read_csv(documents_filename,index_col=\"document_id\")\n",
    "Y=documents[\"label\"]\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.018607Z",
     "start_time": "2018-10-11T14:31:52.000607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../raw/C50/C50test/ScottHillis/373999newsML...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../raw/C50/C50test/ScottHillis/348602newsML...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../raw/C50/C50test/ScottHillis/387913newsML...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../raw/C50/C50test/ScottHillis/392527newsML...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../raw/C50/C50test/ScottHillis/417664newsML...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      filename        label\n",
       "document_id                                                                \n",
       "0            ../../raw/C50/C50test/ScottHillis/373999newsML...  ScottHillis\n",
       "1            ../../raw/C50/C50test/ScottHillis/348602newsML...  ScottHillis\n",
       "2            ../../raw/C50/C50test/ScottHillis/387913newsML...  ScottHillis\n",
       "3            ../../raw/C50/C50test/ScottHillis/392527newsML...  ScottHillis\n",
       "4            ../../raw/C50/C50test/ScottHillis/417664newsML...  ScottHillis"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_documents_filename=data_dir+\"/C50_test_documents.csv\"\n",
    "test_documents=pd.read_csv(test_documents_filename,index_col=\"document_id\")\n",
    "Y_test=test_documents[\"label\"]\n",
    "test_documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.048607Z",
     "start_time": "2018-10-11T14:31:52.020607Z"
    }
   },
   "outputs": [],
   "source": [
    "set_vectorizer_filename=   data_dir+\"/set_vectorizer.p\"\n",
    "set_vectorizer=pickle.load(open(set_vectorizer_filename,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.069607Z",
     "start_time": "2018-10-11T14:31:52.050607Z"
    }
   },
   "outputs": [],
   "source": [
    "# get dimension to word mapping\n",
    "index_2_word=set_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.095607Z",
     "start_time": "2018-10-11T14:31:52.071607Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_features_filename=data_dir+\"/\"+\"set_features.p\"\n",
    "set_features=pickle.load(open(set_features_filename,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.118607Z",
     "start_time": "2018-10-11T14:31:52.097607Z"
    }
   },
   "outputs": [],
   "source": [
    "set_test_features_filename=data_dir+\"/\"+\"set_test_features.p\"\n",
    "set_test_features=pickle.load(open(set_test_features_filename,\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting words relevant for Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes assumes uses each word in corpus independently to build a classifier.\n",
    "\n",
    "We would like to build a classifier that, somehow,  takes into account **interactions between words**. \n",
    "\n",
    "The difficulty is that the vocabulary size $\\approx 30k$ is too large to model all possible interactions. \n",
    "\n",
    "We need a procedure to **select** only a few relevant **features** (words).\n",
    "\n",
    "The idea is that we will select words are are **least independently distributed** over all the possible classes.\n",
    "\n",
    "We are using text as an example here, but the same ideas apply in pretty much the same way to any classification problem with **categorical** variables.\n",
    "\n",
    "For numerical values, a $R^2$ test plays the same role "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\chi^2$ Test of Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With represent a document  $i$ as the vector of word counts $X_{i,d}$.  Where the index $i$ runs documents $i=1,\\dots N$, and  $d$ runs over words (the corpus vocabulary) $d=1,\\dots V$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.155607Z",
     "start_time": "2018-10-11T14:31:52.120607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 2500\n",
      "V 28060\n"
     ]
    }
   ],
   "source": [
    "X=set_features\n",
    "N=X.shape[0]\n",
    "V=X.shape[1]\n",
    "print(\"N\",N)\n",
    "print(\"V\",X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document classes are one hot-encoded $Z_{i,k}$ , for $k=1,\\cdots K$, where $K$ is the number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.188607Z",
     "start_time": "2018-10-11T14:31:52.157607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 50)\n"
     ]
    }
   ],
   "source": [
    "dummies=pd.get_dummies(Y,prefix=\"\",prefix_sep=\"\",sparse=True)\n",
    "labels=dummies.columns\n",
    "Z=dummies.to_coo() # Most entries are zero, so we want Z represented as a sparse matrix\n",
    "             # if you remove this line, calculations become very slow\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency with which documents are assigned to class $k$ is\n",
    "$$\n",
    "        f_k = \\frac{1}{N}\\sum_{i=1}^N Z_{i,k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.202607Z",
     "start_time": "2018-10-11T14:31:52.190607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=Z.sum(axis=0)/N\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of times that word $d$ appears in documents of class $k$ is \n",
    "\n",
    "$$\n",
    "    n_{k,d} = \\sum_i Z_{i,k} X_{i,d} = Z^T X.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.304607Z",
     "start_time": "2018-10-11T14:31:52.204607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 28060)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts=Z.T.dot(X)\n",
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total times a word appears in the corpus is \n",
    "\n",
    "$$\n",
    "    c_{d} = \\sum_k n_{k,d}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.382607Z",
     "start_time": "2018-10-11T14:31:52.306607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28060)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_counts=word_counts.sum(axis=0)\n",
    "corpus_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If word counts are **independently distributed** over the document classes $k$ we would expect a per class  word count of\n",
    "$$\n",
    "    e_{k,d} = f_k \\,c_{d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.455607Z",
     "start_time": "2018-10-11T14:31:52.384607Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 28060)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected=(f.T*corpus_counts)\n",
    "expected=np.asarray(expected) # this object is a \"matrix\" but we want a regular array\n",
    "expected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If word $d$ was distributed independently over the classes $k$\n",
    "the following quantity\n",
    "$$\n",
    "   C^2_d= \\sum_k \\frac{\\left(n_{k,d} - e_{k,d}\\right)^2}{e_{k,d}}\n",
    "$$\n",
    "   \n",
    "would be distributed as $C^2_d \\sim \\chi^2_{K-1}$, a  $\\chi^2$ variate with $K-1$ degrees of freedom.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.504607Z",
     "start_time": "2018-10-11T14:31:52.457607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 28060)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference between actual and expected word counts\n",
    "df=(word_counts - expected)\n",
    "df=np.asarray(df) # again we don't want matrix but array objects\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.604607Z",
     "start_time": "2018-10-11T14:31:52.506607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 28060)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalized square difference \n",
    "df2=df*df/expected\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.613607Z",
     "start_time": "2018-10-11T14:31:52.607607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28060,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of squares of normalized gaussians\n",
    "C2=df2.sum(axis=0)\n",
    "C2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection using $\\chi^2$ test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  simply pick the the \n",
    "$F$ words that are the **least independently distributed** as measured by the $\\chi^2$ statistic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.637607Z",
     "start_time": "2018-10-11T14:31:52.615607Z"
    }
   },
   "outputs": [],
   "source": [
    "F=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.668607Z",
     "start_time": "2018-10-11T14:31:52.642607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx=C2.argsort()[-F:] # pick largest F score indexes\n",
    "best_X=X[:,best_idx] # pick F best features for each document\n",
    "best_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look  at the 10 most significant words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.686607Z",
     "start_time": "2018-10-11T14:31:52.670607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index C2\n",
      "cocoa 9729 1862.4999999999993\n",
      "pragu 21054 1869.2881355932175\n",
      "moscow 18598 1885.3333333333344\n",
      "automak 7263 1915.0439560439534\n",
      "toronto 25827 1944.1908396946528\n",
      "5017 4322 2008.9999999999975\n",
      "abidjan 6022 2008.9999999999986\n",
      "colombia 9803 2134.9999999999973\n",
      "czech 10642 2300.324503311257\n",
      "tel+44 25302 2352.0000000000014\n"
     ]
    }
   ],
   "source": [
    "# Let's see the most significant words\n",
    "print(\"word\",\"index\",\"C2\")\n",
    "for idx in best_idx[-10:]:\n",
    "    print(index_2_word[idx],idx,C2[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.714607Z",
     "start_time": "2018-10-11T14:31:52.688607Z"
    }
   },
   "outputs": [],
   "source": [
    "best_indexes=reversed(list(best_idx[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.789607Z",
     "start_time": "2018-10-11T14:31:52.716607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tel+44 2352.0000000000014\n",
      "\t JimGilchrist   (0, 0)\t48\n",
      "czech 2300.324503311257\n",
      "\t AlanCrosby   (0, 0)\t49\n",
      "\t JanLopatka   (0, 0)\t50\n",
      "\t JohnMastrini   (0, 0)\t50\n",
      "\t MureDickie   (0, 0)\t1\n",
      "\t TanEeLyn   (0, 0)\t1\n",
      "colombia 2134.9999999999973\n",
      "\t JaneMacartney   (0, 0)\t1\n",
      "\t KarlPenhaul   (0, 0)\t49\n",
      "\t MatthewBunce   (0, 0)\t1\n",
      "\t MichaelConnor   (0, 0)\t2\n",
      "\t RogerFillion   (0, 0)\t1\n",
      "\t ScottHillis   (0, 0)\t1\n",
      "abidjan 2008.9999999999986\n",
      "\t MatthewBunce   (0, 0)\t41\n",
      "5017 2008.9999999999975\n",
      "\t JimGilchrist   (0, 0)\t41\n",
      "toronto 1944.1908396946528\n",
      "\t AlanCrosby   (0, 0)\t4\n",
      "\t DarrenSchuettler   (0, 0)\t44\n",
      "\t HeatherScoffield   (0, 0)\t37\n",
      "\t LydiaZajc   (0, 0)\t46\n",
      "automak 1915.0439560439534\n",
      "\t BradDorfman   (0, 0)\t3\n",
      "\t DavidLawder   (0, 0)\t46\n",
      "\t KevinDrawbaugh   (0, 0)\t2\n",
      "\t MichaelConnor   (0, 0)\t1\n",
      "\t ToddNissen   (0, 0)\t39\n",
      "moscow 1885.3333333333344\n",
      "\t JanLopatka   (0, 0)\t1\n",
      "\t JaneMacartney   (0, 0)\t1\n",
      "\t LynnleyBrowning   (0, 0)\t43\n",
      "\t ScottHillis   (0, 0)\t1\n",
      "\t WilliamKazer   (0, 0)\t2\n",
      "pragu 1869.2881355932175\n",
      "\t AlanCrosby   (0, 0)\t37\n",
      "\t JanLopatka   (0, 0)\t36\n",
      "\t JohnMastrini   (0, 0)\t45\n",
      "cocoa 1862.4999999999993\n",
      "\t LynnleyBrowning   (0, 0)\t1\n",
      "\t MatthewBunce   (0, 0)\t39\n"
     ]
    }
   ],
   "source": [
    "for idx in best_indexes:\n",
    "    print(index_2_word[idx],C2[idx])\n",
    "    for l,count in enumerate(word_counts[:,idx]):\n",
    "        if count>0:\n",
    "            print(\"\\t\",labels[l],count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few comments about what information we are picking up\n",
    "- Jim Gilchrist seems to work on the UK (telephone country code +44). The reason we pick him up is that most of his articles leave the phone number in an **idiosyncratic** fashion `tel+44` that does not get broken up by our **tokenization** algorithm.\n",
    "- Alan Crosby seems to the Reuters correspondent to the Czech republic. His articles cover Czech's stock market, politics, news, etc.. We are are picking up the articles **geographical references**, not anything about the *authors writing style*.\n",
    "- JanLopakta seems to cover only the Czech's republic stock market and economics. JhonMastrini covers Czech's economic and politic news (but not the stock market). The fact that there are 3 our of 50 authors dedicated to Czech's news indicates there are **geographical** imbalances on how this dataset was collected.\n",
    "- The fact that Colombia, Toronto, Moscow, Abidjan (capital of Ivory Coast, in West Africa), cocoa are other highly informative words seems to indicate different authors cover different subject matter ( a geographical region, a commodity) and that is what we **most likely learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le'ts look at their word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.799607Z",
     "start_time": "2018-10-11T14:31:52.791607Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 37,  0,  0,  4,  0,  0,  0, 49,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  3,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 44,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, 46,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 37,  0,  0,  0,  0,  0],\n",
       "        [ 0, 36,  1,  0,  0,  0,  0,  0, 50,  0],\n",
       "        [ 0,  0,  1,  0,  0,  0,  0,  1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0, 41,  0,  0,  0, 48],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 45,  0,  0,  0,  0,  0,  0, 50,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0, 49,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  2,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 46,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  0, 43,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [39,  0,  0,  0,  0,  0, 41,  1,  0,  0],\n",
       "        [ 0,  0,  0,  1,  0,  0,  0,  2,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  1,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  1,  0,  0,  0,  0,  1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  1,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, 39,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  2,  0,  0,  0,  0,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the word counts\n",
    "word_counts[:,best_idx[-10:]].todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the highest score **tel+44** looks suspicious (some kind of signature).\n",
    "\n",
    "We are learning from the data, but maybe not what we thought we were.\n",
    "\n",
    "Most of they important words are highly concentrated, and we may be **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at lest significant words that still meet our threshold. \n",
    "Words with ranking on the order of $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.823607Z",
     "start_time": "2018-10-11T14:31:52.801607Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index C2\n",
      "paramilitari 20227 544.6923076923075\n",
      "anti-drug 6800 544.6923076923075\n",
      "408-8787 3805 544.6923076923076\n",
      "encrypt 12026 548.6666666666674\n",
      "metal 18077 549.7826086956517\n",
      "leader 16761 553.2553191489361\n",
      "quarantin 21563 554.2222222222226\n",
      "polit 20879 556.4143302180684\n",
      "daewoo 10663 557.5714285714279\n",
      "foreign 13118 557.854700854701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 0,  0,  0, 14,  0,  0,  0,  1,  0,  9],\n",
       "        [ 0,  0,  0,  0,  0,  3,  0, 11,  1, 22],\n",
       "        [ 0,  0,  0,  0,  2,  2,  0,  1,  0,  3],\n",
       "        [ 0,  0,  0,  0,  0, 34,  0, 28,  0, 20],\n",
       "        [ 0,  0,  0,  0,  1,  4,  0,  0,  0,  5],\n",
       "        [ 0,  0,  1,  0,  0,  4,  0,  2,  0,  1],\n",
       "        [ 0,  0,  0,  0,  1,  4,  0,  1,  0,  9],\n",
       "        [ 0,  0,  0,  0, 21, 15,  0,  1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  5,  0,  0,  0,  2],\n",
       "        [ 0,  0,  0,  0,  0,  7,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  1,  0,  5,  0, 16],\n",
       "        [ 0,  0,  0,  0,  5,  2,  0,  6,  0, 29],\n",
       "        [ 0,  0,  0,  0, 10,  3,  0,  6,  0,  2],\n",
       "        [ 0,  0,  0,  0,  0, 12,  0, 13,  0, 27],\n",
       "        [ 0,  0,  0,  0,  0, 23,  0, 24,  0, 27],\n",
       "        [ 0,  0,  0,  0,  1,  1,  1,  1,  0,  1],\n",
       "        [ 0,  0,  0,  0,  0,  6,  0, 12,  0,  0],\n",
       "        [ 0,  0,  0,  0,  1,  0,  0,  2,  0,  3],\n",
       "        [ 0,  0,  0,  0,  0, 13,  0, 21,  0, 32],\n",
       "        [ 0,  0,  0,  0,  0,  1,  0,  2,  1,  5],\n",
       "        [12, 12,  0,  0,  0, 17,  0, 18,  0, 14],\n",
       "        [ 0,  0,  0,  0,  0,  4,  0,  2,  0,  5],\n",
       "        [ 0,  0,  0,  0,  0,  3,  0,  0,  0,  3],\n",
       "        [ 0,  0,  0,  0,  6,  2,  0,  0,  0,  9],\n",
       "        [ 0,  0,  0,  0,  0,  9,  0,  3,  0,  6],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  2,  0,  1],\n",
       "        [ 0,  0,  0,  0, 21,  0,  0,  3,  0,  2],\n",
       "        [ 0,  0,  0,  0, 14,  3, 14,  4,  0,  5],\n",
       "        [ 0,  0,  0,  0, 15,  4,  0, 10,  0, 20],\n",
       "        [ 0,  0,  0,  0,  0,  8,  0,  3, 15, 10],\n",
       "        [ 0,  0,  0,  0,  6,  0,  0,  0,  0,  3],\n",
       "        [ 0,  0,  0,  0,  0,  5,  0,  0,  0,  0],\n",
       "        [ 1,  0,  0,  0,  0,  5,  0,  2,  0,  7],\n",
       "        [ 0,  0,  0,  0,  0,  2,  0,  1,  0,  3],\n",
       "        [ 0,  0,  0,  0,  0, 26,  0, 18,  0, 31],\n",
       "        [ 0,  0,  0,  0,  0,  8,  0,  0,  0,  5],\n",
       "        [ 0,  0, 12,  0,  1,  3,  0,  0,  0,  1],\n",
       "        [ 0,  0,  0,  0,  0, 39,  0, 37,  0, 16],\n",
       "        [ 0,  0,  0,  0,  1,  1,  0,  4,  4,  1],\n",
       "        [ 0,  0,  0,  0,  0,  5,  0,  1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  6,  0,  6],\n",
       "        [ 0,  0,  0,  2,  0,  7,  0,  4,  0,  1],\n",
       "        [ 0,  0,  0,  0,  0,  8,  0, 19,  0, 22],\n",
       "        [ 0,  1,  0,  0,  3, 21,  0, 15,  0, 30],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  2, 30,  0, 14,  0, 21],\n",
       "        [ 0,  0,  0,  2,  1,  9,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  3,  0,  2,  0,  2],\n",
       "        [ 0,  0,  0,  0,  2,  0,  0,  0,  0,  2],\n",
       "        [ 0,  0,  0,  0,  1, 14,  3, 16,  0, 29]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the least significant of the most words 300 scores\n",
    "print(\"word\",\"index\",\"C2\")\n",
    "for idx in best_idx[:10]:\n",
    "    print(index_2_word[idx],idx,C2[idx])\n",
    "word_counts[:,best_idx[:10]].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words seem distributed over a few authors, as we would like.\n",
    "Others, however, are just only used by one author. \n",
    "\n",
    "A phone number inserted itself into the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's look at words with very low ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.850607Z",
     "start_time": "2018-10-11T14:31:52.825607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index C2\n",
      "said 22797 0.47381144238517414\n",
      "ha 14193 34.33067729083664\n",
      "wa 27115 37.04068117313152\n",
      "thi 25502 38.90109890109892\n",
      "jockey 15991 40.00000000000001\n",
      "valid 26800 41.0\n",
      "recipi 21913 41.0\n",
      "9.4 5742 41.9999999999999\n",
      "encount 12023 41.999999999999915\n",
      "hell 14540 41.99999999999995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[50, 29, 32, 34,  0,  0,  1,  0,  1,  0],\n",
       "        [49, 39, 47, 33,  1,  1,  1,  0,  0,  0],\n",
       "        [50, 38, 50, 44,  0,  1,  0,  1,  0,  1],\n",
       "        [50, 50, 50, 29,  1,  0,  0,  0,  0,  0],\n",
       "        [50, 35, 47, 37,  0,  0,  1,  1,  1,  0],\n",
       "        [50, 38, 44, 36,  0,  1,  0,  0,  0,  0],\n",
       "        [50, 36, 38, 41,  0,  0,  1,  0,  0,  0],\n",
       "        [50, 42, 39, 30,  0,  0,  0,  0,  0,  0],\n",
       "        [48, 43, 40, 33,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 39, 41, 38,  0,  0,  0,  0,  1,  1],\n",
       "        [50, 40, 41, 35,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 42, 44, 41,  0,  1,  1,  0,  1,  1],\n",
       "        [48, 46, 47, 37,  0,  0,  0,  0,  0,  1],\n",
       "        [50, 38, 45, 37,  0,  0,  0,  1,  0,  0],\n",
       "        [50, 46, 48, 39,  0,  0,  0,  1,  1,  0],\n",
       "        [47, 32, 29, 28,  0,  0,  0,  1,  0,  0],\n",
       "        [50, 45, 49, 38,  1,  0,  0,  0,  0,  0],\n",
       "        [49, 45, 35, 45,  1,  0,  0,  0,  0,  0],\n",
       "        [50, 43, 43, 39,  0,  0,  0,  1,  0,  0],\n",
       "        [48, 32, 44, 29,  0,  1,  0,  0,  1,  0],\n",
       "        [50, 38, 47, 32,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 47, 46, 35,  0,  0,  0,  1,  0,  0],\n",
       "        [50, 31, 38, 39,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 47, 47, 40,  0,  0,  0,  0,  0,  0],\n",
       "        [49, 49, 40, 38,  1,  1,  0,  0,  0,  0],\n",
       "        [50, 37, 30, 31,  0,  0,  0,  0,  1,  0],\n",
       "        [50, 28, 43, 29,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 42, 47, 41,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 38, 45, 41,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 46, 46, 36,  0,  1,  0,  0,  0,  0],\n",
       "        [49, 41, 39, 35,  0,  0,  0,  0,  0,  1],\n",
       "        [49, 37, 44, 30,  0,  1,  0,  0,  0,  0],\n",
       "        [50, 42, 44, 35,  0,  0,  0,  0,  0,  1],\n",
       "        [50, 34, 39, 28,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 48, 49, 37,  0,  0,  1,  0,  0,  0],\n",
       "        [50, 37, 42, 46,  0,  0,  0,  0,  0,  1],\n",
       "        [50, 40, 39, 43,  0,  0,  0,  0,  0,  0],\n",
       "        [49, 48, 43, 42,  1,  0,  0,  0,  0,  0],\n",
       "        [49, 42, 46, 37,  0,  0,  0,  0,  0,  0],\n",
       "        [49, 42, 40, 42,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 38, 24, 27,  0,  1,  0,  0,  0,  1],\n",
       "        [50, 32, 39, 46,  1,  0,  1,  0,  0,  0],\n",
       "        [50, 40, 37, 42,  1,  0,  1,  0,  0,  0],\n",
       "        [49, 44, 43, 38,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 41, 46, 35,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 43, 48, 36,  1,  0,  0,  0,  0,  0],\n",
       "        [50, 44, 40, 38,  0,  0,  1,  0,  1,  0],\n",
       "        [50, 33, 35, 31,  1,  0,  0,  1,  0,  0],\n",
       "        [50, 38, 48, 23,  0,  0,  0,  0,  0,  0],\n",
       "        [50, 43, 47, 44,  0,  0,  0,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_least=10\n",
    "# place the indexes of the F smallest C2 scores at the\n",
    "# begining of partition array\n",
    "partition_worst=np.argpartition(C2, F_least) \n",
    "# we are only interest on the F largest C2 scores\n",
    "worst_idx = partition_worst[:F_least] \n",
    "# get the C2 scores of the best indexes.\n",
    "worst_C2=C2[worst_idx]\n",
    "# Not strickly necessary, but it would be nice to have\n",
    "# the test scores sorted by C2 score\n",
    "worst_idx=worst_idx[np.argsort(worst_C2)]\n",
    "\n",
    "\n",
    "print(\"word\",\"index\",\"C2\")\n",
    "for idx in worst_idx[:10]:\n",
    "    print(index_2_word[idx],idx,C2[idx])\n",
    "word_counts[:,worst_idx[:10]].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the words are evenly distributed, others are just very rare in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comparsion to Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.934607Z",
     "start_time": "2018-10-11T14:31:52.852607Z"
    }
   },
   "outputs": [],
   "source": [
    "sk_C2,prob=chi2(X,Z) #chi2 from sklearn.feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our calculations agree with 'sklearn.feature_selection.chi2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.941607Z",
     "start_time": "2018-10-11T14:31:52.936607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8609209661787304e-26"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(sk_C2,C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the $F$ more relevant features\n",
    "\n",
    "Consult 'sklearn.feature_selection' for other possible choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:52.968607Z",
     "start_time": "2018-10-11T14:31:52.943607Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer=SelectKBest(chi2,F) # get the best F based on chi2 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.077607Z",
     "start_time": "2018-10-11T14:31:52.970607Z"
    }
   },
   "outputs": [],
   "source": [
    "sk_Xt=transformer.fit_transform(X,Z) # Return X with only the selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the indeces of the features selected by 'SelectKBest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.084607Z",
     "start_time": "2018-10-11T14:31:53.079607Z"
    }
   },
   "outputs": [],
   "source": [
    "sk_indeces=transformer.get_support(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a bit of set algebra to compare our indeces to 'skelearn''s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.106607Z",
     "start_time": "2018-10-11T14:31:53.086607Z"
    }
   },
   "outputs": [],
   "source": [
    "sk_idx_set=set(sk_indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.131607Z",
     "start_time": "2018-10-11T14:31:53.108607Z"
    }
   },
   "outputs": [],
   "source": [
    "best_idx_set=set(best_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.152607Z",
     "start_time": "2018-10-11T14:31:53.133607Z"
    }
   },
   "outputs": [],
   "source": [
    "diffs=sk_idx_set.symmetric_difference(best_idx_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is  two words wit the same $\\chi^2$ score.\n",
    "We broke the tie differently from 'sklear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.179607Z",
     "start_time": "2018-10-11T14:31:53.154607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21514 544.6923076923073 544.6923076923076\n",
      "6800 544.6923076923075 544.6923076923076\n"
     ]
    }
   ],
   "source": [
    "for idx in diffs:\n",
    "    print(idx,C2[idx],sk_C2[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Comparison on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.202607Z",
     "start_time": "2018-10-11T14:31:53.181607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 2500 2500\n",
      "V 28060 28060\n"
     ]
    }
   ],
   "source": [
    "X_test=set_test_features\n",
    "N_test=X_test.shape[0]\n",
    "V_test=X_test.shape[1]\n",
    "print(\"N\",N_test,N)\n",
    "print(\"V\",V_test,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.240607Z",
     "start_time": "2018-10-11T14:31:53.206607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 50)\n"
     ]
    }
   ],
   "source": [
    "dummies_test=pd.get_dummies(Y_test,prefix=\"\",prefix_sep=\"\",sparse=True)\n",
    "labels_test=dummies_test.columns\n",
    "Z_test=dummies_test.to_coo() # Most entries are zero, so we want Z represented as a sparse matrix\n",
    "             # if you remove this line, calculations become very slow\n",
    "print(Z_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:33:57.292607Z",
     "start_time": "2018-10-11T14:33:57.280607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 28060)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_test=Z_test.T.dot(X_test)\n",
    "word_counts_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:34:44.084607Z",
     "start_time": "2018-10-11T14:34:43.943607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tel+44\n",
      "\t JimGilchrist 48 42\n",
      "czech\n",
      "\t AlanCrosby 49 41\n",
      "\t HeatherScoffield 0 1\n",
      "\t JanLopatka 50 50\n",
      "\t JohnMastrini 50 50\n",
      "\t MureDickie 1 0\n",
      "\t SimonCowell 0 1\n",
      "\t TanEeLyn 1 0\n",
      "colombia\n",
      "\t JaneMacartney 1 0\n",
      "\t KarlPenhaul 49 49\n",
      "\t KirstinRidley 0 3\n",
      "\t MatthewBunce 1 0\n",
      "\t MichaelConnor 2 0\n",
      "\t RogerFillion 1 0\n",
      "\t ScottHillis 1 0\n",
      "abidjan\n",
      "\t MatthewBunce 41 40\n",
      "5017\n",
      "\t JimGilchrist 41 38\n",
      "toronto\n",
      "\t AlanCrosby 4 0\n",
      "\t DarrenSchuettler 44 46\n",
      "\t HeatherScoffield 37 23\n",
      "\t JimGilchrist 0 5\n",
      "\t KarlPenhaul 0 1\n",
      "\t LydiaZajc 46 48\n",
      "\t PeterHumphrey 0 1\n",
      "automak\n",
      "\t BradDorfman 3 0\n",
      "\t DavidLawder 46 38\n",
      "\t KevinDrawbaugh 2 0\n",
      "\t MichaelConnor 1 0\n",
      "\t RobinSidel 0 1\n",
      "\t ToddNissen 39 27\n",
      "moscow\n",
      "\t JanLopatka 1 1\n",
      "\t JaneMacartney 1 0\n",
      "\t JohnMastrini 0 5\n",
      "\t LynnleyBrowning 43 40\n",
      "\t MarcelMichelson 0 2\n",
      "\t PeterHumphrey 0 1\n",
      "\t ScottHillis 1 1\n",
      "\t WilliamKazer 2 0\n",
      "pragu\n",
      "\t AlanCrosby 37 44\n",
      "\t JanLopatka 36 42\n",
      "\t JohnMastrini 45 41\n",
      "cocoa\n",
      "\t KarlPenhaul 0 1\n",
      "\t LynnleyBrowning 1 0\n",
      "\t MatthewBunce 39 37\n",
      "\t PatriciaCommins 0 1\n",
      "\t TimFarrand 0 1\n"
     ]
    }
   ],
   "source": [
    "best_indexes=reversed(list(best_idx[-10:]))\n",
    "for idx in best_indexes:\n",
    "    print(index_2_word[idx])\n",
    "    for l,counts in enumerate(zip(word_counts[:,idx],word_counts_test[:,idx])):\n",
    "        count,count_test=counts\n",
    "        if count>0 or count_test>0:\n",
    "            print(\"\\t\",labels[l],count[0,0],count_test[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:31:53.427607Z",
     "start_time": "2018-10-11T14:31:53.424607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(*best_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
